{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries and environments\n",
    "import spacy\n",
    "from nltk.corpus import gutenberg\n",
    "from spacy import displacy\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations for rule 2 = 108\n"
     ]
    }
   ],
   "source": [
    "###global variable declarations###\n",
    "\n",
    "#for rule 1\n",
    "#----\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "#for rule 2\n",
    "#----\n",
    "# Define the keywords that will be used to extract the imperative sentences\n",
    "# Rules are identified from the paper - https://www.aclweb.org/anthology/W14-2117.pdf\n",
    "# 1. \"find those sentences with a verb (in its base form) as the root \n",
    "#     in the phrase structure and this particular verb has no \n",
    "#.    subject child in the dependency structure\" \n",
    "#.    Example: You must first discuss the matter there, and you need to be specific”\n",
    "# 2. \"recognize the use of a personal pronoun or noun (e.g., “you”, “we”,\n",
    "#.     or a username) followed by a modal verb (e.g., “should”, “must”, “need”) as an imperative\"\n",
    "\n",
    "\n",
    "# Source: https://www.myenglishpages.com/site_php_files/grammar-lesson-modals.php\n",
    "modal_verbs = ['can', 'could', 'may', 'might', 'will', 'would', 'shall','should', 'must', 'ought', 'dare', 'had better', 'ask']\n",
    "modal_verbs = ['can', 'may', 'might', 'will', 'would', 'shall','should', 'must', 'ought', 'dare', 'had better', 'ask']\n",
    "\n",
    "# Source: https://grammar.yourdictionary.com/parts-of-speech/pronouns/list-of-personal-pronouns.html\n",
    "# personal_pronoun = ['I', 'me', 'we', 'us', 'you', 'he', 'she', 'her', 'him', 'it', 'they', 'them']\n",
    "personal_pronoun = ['us', 'you', 'he', 'she', 'her', 'him', 'it', 'they', 'them']\n",
    "\n",
    "# Create combinations of possibilities for a sentence to be imperative\n",
    "combinations = []\n",
    "# form combinations of words and put them as a list\n",
    "for pronoun in personal_pronoun:\n",
    "    for verb in modal_verbs:\n",
    "        combinations.append(pronoun + ' ' + verb)\n",
    "        \n",
    "print('Number of combinations for rule 2 =', len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule 1: Has a verb in its lemma (base) form and is the root and does not have any subject child in it's dependency structure\n",
    "def rule1(sent):\n",
    "    doc = nlp(sent)\n",
    "    cond1 = False\n",
    "    cond2 = True\n",
    "    for token in doc:\n",
    "        if token.dep_=='ROOT' and token.pos_=='VERB' and token.text.lower() == token.lemma_:\n",
    "            cond1 = True\n",
    "            #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "        if token.dep_ == 'nsubj':\n",
    "            cond2 = False   \n",
    "    return (cond1 and cond2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule 2:\n",
    "def rule2(combinations, sentence):\n",
    "    if any(combo in sentence.lower() for combo in combinations):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kevins dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/kevin/tst_data\n",
      "./data/kevin/trn_data\n"
     ]
    }
   ],
   "source": [
    "imp_sents = []\n",
    "nimp_sents = []\n",
    "for fname in os.listdir(\"./data/kevin\"):\n",
    "    if \"data\" in fname:\n",
    "        print (\"./data/kevin/\"+fname)\n",
    "        f = open(\"./data/kevin/\"+fname, \"r\")\n",
    "        for line in f.readlines():\n",
    "            dline = line.strip().split(\"\\t\")\n",
    "            if int(dline[0])==2:\n",
    "                imp_sents.append(dline[1])\n",
    "            else:\n",
    "                nimp_sents.append(dline[1])\n",
    "        #data = pd.read_csv(\"./data/kevin/\"+fname, sep='\\s+', engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_samples = random.sample(nimp_sents, len(imp_sents))\n",
    "neg_labels = [0]*len(neg_samples)\n",
    "pos_samples = imp_sents\n",
    "pos_labels = [1]*len(pos_samples)\n",
    "all_samples = neg_samples+pos_samples\n",
    "all_labels = neg_labels+pos_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame({'labels': all_labels, 'sentences': all_samples})\n",
    "all_df.to_pickle(\"./data/kevin_imp_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for sent in all_samples:\n",
    "    r1_cond = False\n",
    "    r2_cond = False\n",
    "    r1_cond = rule1(sent)\n",
    "    r2_cond = rule2(combinations, sent)\n",
    "    if r1_cond or r2_cond:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7770114942528735"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk.metrics.accuracy_score(all_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.887459807073955, 0.6344827586206897, 0.7399463806970511, None)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk.metrics.precision_recall_fscore_support(all_labels, preds, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non Imperative</th>\n",
       "      <th>Imperative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.715564</td>\n",
       "      <td>0.887460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.634483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fscore</th>\n",
       "      <td>0.804829</td>\n",
       "      <td>0.739946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>435.000000</td>\n",
       "      <td>435.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Non Imperative  Imperative\n",
       "precision        0.715564    0.887460\n",
       "recall           0.919540    0.634483\n",
       "fscore           0.804829    0.739946\n",
       "size           435.000000  435.000000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=sk.metrics.precision_recall_fscore_support(all_labels, preds), index=['precision', 'recall', 'fscore', 'size'], columns=['Non Imperative', 'Imperative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-sad",
   "language": "python",
   "name": "venv-sad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
